{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":13817007,"datasetId":8798681,"databundleVersionId":14574603},{"sourceType":"datasetVersion","sourceId":13817018,"datasetId":8798692,"databundleVersionId":14574616}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Kaggle-Optimized Apple DepthPro Evaluation on KITTI\n# ============================================================\n\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib\nmatplotlib.use(\"Agg\")   # Important for Kaggle speed\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport time\nimport json\nfrom tqdm import tqdm\nfrom transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n\nclass DepthProKITTITester:\n    def __init__(self, device='cuda', use_scale_alignment=False):\n        \"\"\"\n        Kaggle-optimized metric depth evaluation for DepthPro.\n        \"\"\"\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        self.model = None\n        self.image_processor = None\n        self.use_scale_alignment = use_scale_alignment\n\n        print(f\"ðŸš€ Using device: {self.device}\")\n        print(f\"ðŸ“ Scale alignment: {'Enabled' if use_scale_alignment else 'Disabled (metric model)'}\")\n\n    def load_model(self):\n        \"\"\"Load DepthPro with autocast + GPU\"\"\"\n        print(\"ðŸ“¥ Loading Apple DepthPro...\")\n\n        try:\n            self.image_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n            self.model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(self.device)\n            self.model.eval()\n            print(\"âœ… Model loaded successfully!\")\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Error loading model: {e}\")\n            return False\n\n    def load_kitti_depth(self, depth_path):\n        \"\"\"Load KITTI ground truth depth (fast)\"\"\"\n        depth = cv2.imread(str(depth_path), cv2.IMREAD_ANYDEPTH).astype(np.float32)\n        depth /= 256.0\n        valid_mask = depth > 0\n        return depth, valid_mask\n\n    def predict_depth(self, image_path):\n        \"\"\"Fast inference using autocast on GPU\"\"\"\n        start_time = time.time()\n\n        image = Image.open(str(image_path)).convert(\"RGB\")\n        original_size = (image.height, image.width)\n\n        # GPU preprocessing\n        inputs = self.image_processor(images=image, return_tensors=\"pt\").to(self.device)\n\n        # ðŸ”¥ Mixed precision for 2â€“3Ã— speedup\n        with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n            outputs = self.model(**inputs)\n\n        post = self.image_processor.post_process_depth_estimation(\n            outputs, target_sizes=[original_size]\n        )\n\n        depth_map = post[0][\"predicted_depth\"]\n\n        if isinstance(depth_map, torch.Tensor):\n            depth_map = depth_map.detach().cpu().numpy()\n\n        inference_time = time.time() - start_time\n        return depth_map, inference_time, {}\n\n    def align_prediction_to_gt(self, pred, gt, valid_mask):\n        pred_valid = pred[valid_mask]\n        gt_valid = gt[valid_mask]\n\n        A = np.vstack([pred_valid, np.ones(len(pred_valid))]).T\n        s, t = np.linalg.lstsq(A, gt_valid, rcond=None)[0]\n\n        return s * pred + t, s, t\n\n    def compute_metrics(self, pred, gt, valid_mask):\n        pred = pred[valid_mask]\n        gt = gt[valid_mask]\n\n        pred = np.clip(pred, 1e-3, None)\n        gt = np.clip(gt, 1e-3, None)\n\n        thresh = np.maximum(gt / pred, pred / gt)\n        delta_1 = (thresh < 1.25).mean()\n        delta_2 = (thresh < 1.25**2).mean()\n        delta_3 = (thresh < 1.25**3).mean()\n\n        rmse = np.sqrt(((pred - gt) ** 2).mean())\n        rmse_log = np.sqrt(((np.log(pred) - np.log(gt)) ** 2).mean())\n        mae = np.abs(pred - gt).mean()\n        abs_rel = (np.abs(pred - gt) / gt).mean()\n        sq_rel = (((pred - gt) ** 2) / gt).mean()\n\n        return {\n            'delta_1': float(delta_1),\n            'delta_2': float(delta_2),\n            'delta_3': float(delta_3),\n            'rmse': float(rmse),\n            'rmse_log': float(rmse_log),\n            'mae': float(mae),\n            'abs_rel': float(abs_rel),\n            'sq_rel': float(sq_rel)\n        }\n\n    def visualize_result(self, image_path, pred_depth, gt_depth, valid_mask, metrics, output_path, scale_info):\n            \"\"\"2x2 visualization matching DAV3 style\"\"\"\n            image = np.array(Image.open(image_path).convert('RGB'))\n    \n            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n            # 1. Input image\n            axes[0, 0].imshow(image)\n            axes[0, 0].set_title('Input Image')\n            axes[0, 0].axis('off')\n    \n            # 2. Predicted Depth (Metric)\n            im1 = axes[0, 1].imshow(pred_depth, cmap='plasma', vmin=0, vmax=80)\n            axes[0, 1].set_title('Predicted Depth (Metric)')\n            axes[0, 1].axis('off')\n            plt.colorbar(im1, ax=axes[0, 1], fraction=0.046, label='meters')\n    \n            # 3. Ground Truth depth\n            gt_disp = gt_depth.copy()\n            gt_disp[~valid_mask] = np.nan\n            im2 = axes[1, 0].imshow(gt_disp, cmap='plasma', vmin=0, vmax=80)\n            axes[1, 0].set_title('Ground Truth Depth')\n            axes[1, 0].axis('off')\n            plt.colorbar(im2, ax=axes[1, 0], fraction=0.046, label='meters')\n    \n            # 4. Absolute Error\n            err = np.abs(pred_depth - gt_depth)\n            err[~valid_mask] = np.nan\n            im3 = axes[1, 1].imshow(err, cmap='hot', vmin=0, vmax=10)\n            axes[1, 1].set_title('Absolute Error')\n            axes[1, 1].axis('off')\n            plt.colorbar(im3, ax=axes[1, 1], fraction=0.046, label='meters')\n    \n            # Metrics box\n            text = '\\n'.join([f'{k}: {v:.4f}' for k, v in metrics.items() if k != 'inference_time'])\n            fig.text(0.02, 0.02, text, fontsize=10, family='monospace',\n                     bbox=dict(boxstyle='round', facecolor='wheat'))\n    \n            plt.tight_layout()\n            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n            plt.close()\n    \n            print(f\"   ðŸ’¾ Saved: {output_path}\")\n\n\n    def test_on_kitti(self, image_dir, gt_dir, output_dir, num_samples=20):\n        image_path = Path(image_dir)\n        gt_path = Path(gt_dir)\n        output = Path(output_dir)\n        output.mkdir(parents=True, exist_ok=True)\n\n        image_files = sorted(image_path.glob(\"*.png\"))\n        if not image_files:\n            print(\"âŒ No PNG images found!\")\n            return\n\n        print(f\"ðŸ“ Found {len(image_files)} images.\")\n        image_files = image_files[:num_samples]\n\n        all_metrics = []\n\n        for idx, img_path in tqdm(enumerate(image_files), total=len(image_files)):\n            pred, t, _ = self.predict_depth(img_path)\n\n            gt_file = gt_path / img_path.name\n            gt_depth, valid_mask = self.load_kitti_depth(gt_file)\n\n            pred = cv2.resize(pred, (gt_depth.shape[1], gt_depth.shape[0]))\n\n            if self.use_scale_alignment:\n                pred, s, shift = self.align_prediction_to_gt(pred, gt_depth, valid_mask)\n\n            metrics = self.compute_metrics(pred, gt_depth, valid_mask)\n            metrics['inference_time'] = t\n            all_metrics.append(metrics)\n            vis_path = output / f\"result_{idx:03d}_{img_path.stem}.png\"\n            self.visualize_result(\n                img_path,\n                pred,\n                gt_depth,\n                valid_mask,\n                metrics,\n                vis_path,\n                scale_info={'scale':1.0, 'shift':0.0}\n            )\n\n\n        # Save metrics\n        results_path = output / \"metrics.json\"\n        json.dump({\n            \"average\": {k: float(np.mean([m[k] for m in all_metrics])) for k in all_metrics[0]},\n            \"per_image\": all_metrics\n        }, open(results_path, \"w\"), indent=2)\n\n        print(f\"ðŸ’¾ Saved metrics to: {results_path}\")\n\n        \n\n\n# =====================================================\n# MAIN (Kaggle paths)\n# =====================================================\n\nIMAGE_DIR = \"/kaggle/input/images-test\"\nGT_DIR = \"/kaggle/input/ground-truth\"\nOUTPUT_DIR = \"/kaggle/working/depthpro_results\"\nNUM_SAMPLES = 20\n\ntester = DepthProKITTITester(device=\"cuda\", use_scale_alignment=False)\n\nif tester.load_model():\n    tester.test_on_kitti(IMAGE_DIR, GT_DIR, OUTPUT_DIR, NUM_SAMPLES)\n\nprint(\"\\nâœ… Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:03:45.912575Z","iopub.execute_input":"2025-11-21T13:03:45.913292Z","iopub.status.idle":"2025-11-21T13:04:52.603920Z","shell.execute_reply.started":"2025-11-21T13:03:45.913263Z","shell.execute_reply":"2025-11-21T13:04:52.603069Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Using device: cuda\nðŸ“ Scale alignment: Disabled (metric model)\nðŸ“¥ Loading Apple DepthPro...\nâœ… Model loaded successfully!\nðŸ“ Found 20 images.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_48/26217687.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n  5%|â–Œ         | 1/20 [00:03<00:59,  3.15s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_000_1.png\n","output_type":"stream"},{"name":"stderr","text":" 10%|â–ˆ         | 2/20 [00:06<00:54,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_001_10.png\n","output_type":"stream"},{"name":"stderr","text":" 15%|â–ˆâ–Œ        | 3/20 [00:09<00:51,  3.02s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_002_11.png\n","output_type":"stream"},{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 4/20 [00:12<00:48,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_003_12.png\n","output_type":"stream"},{"name":"stderr","text":" 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:15<00:45,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_004_13.png\n","output_type":"stream"},{"name":"stderr","text":" 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:18<00:42,  3.06s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_005_14.png\n","output_type":"stream"},{"name":"stderr","text":" 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:21<00:39,  3.07s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_006_15.png\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:24<00:36,  3.08s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_007_16.png\n","output_type":"stream"},{"name":"stderr","text":" 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:27<00:33,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_008_17.png\n","output_type":"stream"},{"name":"stderr","text":" 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:30<00:30,  3.03s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_009_18.png\n","output_type":"stream"},{"name":"stderr","text":" 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:33<00:27,  3.03s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_010_19.png\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:36<00:24,  3.03s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_011_2.png\n","output_type":"stream"},{"name":"stderr","text":" 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:39<00:21,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_012_20.png\n","output_type":"stream"},{"name":"stderr","text":" 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:43<00:19,  3.24s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_013_3.png\n","output_type":"stream"},{"name":"stderr","text":" 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:46<00:16,  3.22s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_014_4.png\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:49<00:12,  3.19s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_015_5.png\n","output_type":"stream"},{"name":"stderr","text":" 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:52<00:09,  3.17s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_016_6.png\n","output_type":"stream"},{"name":"stderr","text":" 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:55<00:06,  3.15s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_017_7.png\n","output_type":"stream"},{"name":"stderr","text":" 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:58<00:03,  3.13s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_018_8.png\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:02<00:00,  3.10s/it]","output_type":"stream"},{"name":"stdout","text":"   ðŸ’¾ Saved: /kaggle/working/depthpro_results/result_019_9.png\nðŸ’¾ Saved metrics to: /kaggle/working/depthpro_results/metrics.json\n\nâœ… Done!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\nOUTPUT_DIR = \"/kaggle/working/depthpro_results\"\nZIP_PATH = \"/kaggle/working/depthpro_results\"\n\nprint(\"ðŸ“¦ Creating ZIP file...\")\n\n# Create ZIP of entire directory\nshutil.make_archive(ZIP_PATH, 'zip', OUTPUT_DIR)\n\nprint(f\"âœ… ZIP created at: {ZIP_PATH}.zip\")\nprint(\"You can now download the file from the Kaggle sidebar (Output > Files).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:07:11.678195Z","iopub.execute_input":"2025-11-21T13:07:11.678898Z","iopub.status.idle":"2025-11-21T13:07:12.364948Z","shell.execute_reply.started":"2025-11-21T13:07:11.678866Z","shell.execute_reply":"2025-11-21T13:07:12.364233Z"}},"outputs":[{"name":"stdout","text":"ðŸ“¦ Creating ZIP file...\nâœ… ZIP created at: /kaggle/working/depthpro_results.zip\nYou can now download the file from the Kaggle sidebar (Output > Files).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}